{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Proposal\n",
    "Sandhya Raghavan<br>\n",
    "January 7th 2019\n",
    "\n",
    "## Proposal for Home Credit Default Risk Assessment\n",
    "\n",
    "### Domain Background\n",
    "Every bank has a several strategies to acquire customers for their different lines of business like home loans, auto loans, credit cards etc. Before providing any form of loan to the customers the financial institutions need to assess the Credit Risk.<br>\n",
    "Credit risk is the probable risk of loss resulting from a borrower's failure to repay a loan or meet contractual obligations. Traditionally, it refers to the risk that a lender may not receive the owed principal and interest, which results in an interruption of cash flows and increased costs for collection.<br>\n",
    "For the assessment of Credit Risk, the lenders calculate it based on the borrower's overall ability to repay. To assess credit risk on a consumer loan, lenders look at the five C's:  credit history, capacity to repay, capital, the loan's conditions and associated collateral. Based on the customers value for the five C's they provide a risk score for the customers and decide on whether they should provide the loan or not and if provided at what interset rate.\n",
    "\n",
    "### Problem Statement\n",
    "For the capstone project, I would like to try attempting to solve the Home Credit Default Risk Assessment problem posted as a kaggle competition. Details of the competion are available [here](https://www.kaggle.com/c/home-credit-default-risk)<br>\n",
    "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. As explained in the domain background using the five C's and performing statistical analysis to determine credit risk scores has always been the traditional approach which financial institutions. Home credit would like to use machine learning for this task, so that we could consider as many features as possible not just the five C's to determine whether the customer is a potential credit defaulter or not.\n",
    "\n",
    "### Datasets and Inputs\n",
    "Below are the availble source files provided in the competition\n",
    "- application_{train|test}.csv : This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET). One row represents one loan in the data sample. Most of the fields from this file will be used for this project.\n",
    "- bureau.csv : All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in the sample).For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date. \n",
    "- bureau_balance.csv: Monthly balances of previous credits in Credit Bureau.This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows. \n",
    "- POS_CASH_balance.csv: Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit. This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n",
    "- credit_card_balance.csv:Monthly balance snapshots of previous credit cards that the applicant has with Home Credit. This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n",
    "- previous_application.csv: All previous applications for Home Credit loans of clients who have loans in the sample.There is one row for each previous application related to loans in our data sample.\n",
    "- installments_payments.csv:Repayment history for the previously disbursed credits in Home Credit related to the loans in the sample.There is \n",
    "     1. one row for every payment that was made plus \n",
    "     2. one row each for missed payment.One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n",
    "- HomeCredit_columns_description.csv This file contains descriptions for the columns in the various data files. This is just a data dictionary and we would not be using this for assessment.<BR>\n",
    "All the files above (except the data dictionary) will be used by the project as each have either information of the previous credit accounts or information of previous balances and other details that could prove useful information of the customer and customer's likeliness to pay the loan.\n",
    "\n",
    "### Solution Statement\n",
    "The default risk problem is a classification problem where we would need to classify each customer as a defaulter or not. The solution to this kind of problem is to do an exploratory analysis on the fields provided in each of the input datasets and find the important features. And then using those features we can use machine learning classification algorithms to learn from the training dataset provided. We can measure the correctness of our solution by using the right evaluation metric (which will be discussed in a later section). Based on the metric scores if there is still room for improvement we can try different algorithms for classification or we can try tuning the parameters of the existing model to attain acceptable performance.\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "This being a classification problem we can use the Decision trees clasifier algorithm as the baseline model. Why Decision Tress Classifier?\n",
    "- They are well suited for these kind of problems\n",
    "- They can be understood, they are more of a Whitebox model. Interpretation is very much required in finacial domain.\n",
    "- They can be validated by statistical methods making them more reliable.\n",
    "Using our chosen evaluvation metric, we will be able to measure the performance of our decision tree model which will tell how well the model performs on unseen data. \n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "For classification model one of the most frequently used metric is 'accuracy'. But we cannot use 'accuracy' here because the distribution of the 'Target' feature(Credit defaulter or not) is highly biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEsxJREFUeJzt3X+s3mdd//Hni5UJiGOFlTnbahdo1IFaoNka0QQh2bol2qGbbERXsUkN2YwYNA4Tv1tA8tV8wYUJLpmuriXIXEBY1UJt5pQQYOwMm/2q2OPA7bCxdevcRvii6Xz7x30dd3N29/Rud65z19PnI/nk/tzvz3Vdn+tuTvrK53Nf53NSVUiS1NMLJj0BSdLSZ9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1t2zSEzhenHbaabVmzZpJT0OS/le58847H6uqFUdqZ9g0a9asYWpqatLTkKT/VZL82zjtvI0mSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOJwgsoDf89o5JT0HHoTv/32WTnoI0cV7ZSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7rqFTZLVSW5Lsi/JvUl+o9WvTvKNJHvbdsFQn/ckmU7y1STnDdU3ttp0kiuH6mcmuT3J/iR/meTkVv+e9n66HV/T63NKko6s55XNIeDdVfWjwAbg8iRntWPXVNW6tu0CaMcuAV4DbAT+JMlJSU4CPgKcD5wFXDo0zh+2sdYCTwBbWn0L8ERVvRq4prWTJE1It7Cpqoer6itt/2lgH7Byni6bgJuq6j+q6mvANHB226ar6v6q+k/gJmBTkgBvBj7R+m8HLhwaa3vb/wTwltZekjQBi/KdTbuN9Trg9la6IsldSbYlWd5qK4EHh7rNtNrh6q8A/r2qDs2pf9dY7fiTrb0kaQK6h02SlwKfBN5VVU8B1wGvAtYBDwMfnG06onsdQ32+sebObWuSqSRTBw4cmPdzSJKOXdewSfJCBkHzsar6K4CqeqSqnqmq/wL+lMFtMhhcmawe6r4KeGie+mPAqUmWzal/11jt+MuAg3PnV1XXV9X6qlq/YsWK5/txJUmH0XM1WoAbgH1V9UdD9TOGmr0VuKft7wQuaSvJzgTWAl8G7gDWtpVnJzNYRLCzqgq4Dbio9d8M3DI01ua2fxHw9629JGkClh25yTF7I/DLwN1J9rba7zJYTbaOwW2trwO/BlBV9ya5GbiPwUq2y6vqGYAkVwC7gZOAbVV1bxvvd4Cbkvw+8E8Mwo32+tEk0wyuaC7p+DklSUfQLWyq6vOM/u5k1zx93g+8f0R916h+VXU/z96GG65/B7j4aOYrSerHJwhIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUnfdwibJ6iS3JdmX5N4kv9HqL0+yJ8n+9rq81ZPk2iTTSe5K8vqhsTa39vuTbB6qvyHJ3a3PtUky3zkkSZPR88rmEPDuqvpRYANweZKzgCuBW6tqLXBrew9wPrC2bVuB62AQHMBVwDnA2cBVQ+FxXWs7229jqx/uHJKkCegWNlX1cFV9pe0/DewDVgKbgO2t2Xbgwra/CdhRA18CTk1yBnAesKeqDlbVE8AeYGM7dkpVfbGqCtgxZ6xR55AkTcCifGeTZA3wOuB24PSqehgGgQS8sjVbCTw41G2m1earz4yoM8855s5ra5KpJFMHDhw41o8nSTqC7mGT5KXAJ4F3VdVT8zUdUatjqI+tqq6vqvVVtX7FihVH01WSdBS6hk2SFzIImo9V1V+18iPtFhjt9dFWnwFWD3VfBTx0hPqqEfX5ziFJmoCeq9EC3ADsq6o/Gjq0E5hdUbYZuGWofllblbYBeLLdAtsNnJtkeVsYcC6wux17OsmGdq7L5ow16hySpAlY1nHsNwK/DNydZG+r/S7wB8DNSbYADwAXt2O7gAuAaeDbwDsAqupgkvcBd7R2762qg23/ncCNwIuBz7SNec4hSZqAbmFTVZ9n9PcqAG8Z0b6Ayw8z1jZg24j6FPDaEfXHR51DkjQZPkFAktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrobK2yS3DpOTZKkUZbNdzDJi4CXAKclWQ6kHToF+IHOc5MkLRHzhg3wa8C7GATLnTwbNk8BH+k4L0nSEjJv2FTVh4APJfn1qvrjRZqTJGmJOdKVDQBV9cdJfhJYM9ynqnZ0mpckaQkZK2ySfBR4FbAXeKaVCzBsJElHNFbYAOuBs6qqek5GkrQ0jft7NvcA3380AyfZluTRJPcM1a5O8o0ke9t2wdCx9ySZTvLVJOcN1Te22nSSK4fqZya5Pcn+JH+Z5ORW/572frodX3M085YkLbxxw+Y04L4ku5PsnN2O0OdGYOOI+jVVta5tuwCSnAVcArym9fmTJCclOYnBqrfzgbOAS1tbgD9sY60FngC2tPoW4ImqejVwTWsnSZqgcW+jXX20A1fV547iqmITcFNV/QfwtSTTwNnt2HRV3Q+Q5CZgU5J9wJuBt7c229scr2tjzc73E8CHk8RbgJI0OeOuRvvHBTznFUkuA6aAd1fVE8BK4EtDbWZaDeDBOfVzgFcA/15Vh0a0Xznbp6oOJXmytX9sAT+DJOkojPu4mqeTPNW27yR5JslTx3C+6xisalsHPAx8cPYUI9rWMdTnG+s5kmxNMpVk6sCBA/PNW5L0PIwVNlX1fVV1StteBPwC8OGjPVlVPVJVz1TVfwF/yrO3ymaA1UNNVwEPzVN/DDg1ybI59e8aqx1/GXDwMPO5vqrWV9X6FStWHO3HkSSN6Zie+lxVn2bwnclRSXLG0Nu3MljlBrATuKStJDsTWAt8GbgDWNtWnp3MYBHBzvb9y23ARa3/ZuCWobE2t/2LgL/3+xpJmqxxf6nz54fevoDB793M+x94ko8Db2LwEM8Z4CrgTUnWtb5fZ/DsNarq3iQ3A/cBh4DLq+qZNs4VwG7gJGBbVd3bTvE7wE1Jfh/4J+CGVr8B+GhbZHCQQUBJkiZo3NVoPzu0f4hBUGyar0NVXTqifMOI2mz79wPvH1HfBewaUb+fZ2/DDde/A1w839wkSYtr3NVo7+g9EUnS0jXuarRVST7VngjwSJJPJlnVe3KSpKVh3AUCf87gi/cfYPB7LH/dapIkHdG4YbOiqv68qg617UbAtcKSpLGMGzaPJfml2eeVJfkl4PGeE5MkLR3jhs2vAr8IfJPBb/5fBLhoQJI0lnGXPr8P2NyeY0aSlwMfYBBCkiTNa9wrmx+fDRqAqjoIvK7PlCRJS824YfOCJMtn37Qrm3GviiRJJ7hxA+ODwBeSfILBo2Z+kRG/7S9J0ijjPkFgR5IpBg/fDPDzVXVf15lJkpaMsW+FtXAxYCRJR+2Y/sSAJElHw7CRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1J1hI0nqzrCRJHVn2EiSujNsJEndGTaSpO4MG0lSd4aNJKk7w0aS1F23sEmyLcmjSe4Zqr08yZ4k+9vr8lZPkmuTTCe5K8nrh/psbu33J9k8VH9Dkrtbn2uTZL5zSJImp+eVzY3Axjm1K4Fbq2otcGt7D3A+sLZtW4HrYBAcwFXAOcDZwFVD4XFdazvbb+MRziFJmpBuYVNVnwMOzilvAra3/e3AhUP1HTXwJeDUJGcA5wF7qupgVT0B7AE2tmOnVNUXq6qAHXPGGnUOSdKELPZ3NqdX1cMA7fWVrb4SeHCo3UyrzVefGVGf7xySpAk5XhYIZEStjqF+dCdNtiaZSjJ14MCBo+0uSRrTYofNI+0WGO310VafAVYPtVsFPHSE+qoR9fnO8RxVdX1Vra+q9StWrDjmDyVJmt9ih81OYHZF2WbglqH6ZW1V2gbgyXYLbDdwbpLlbWHAucDuduzpJBvaKrTL5ow16hySpAlZ1mvgJB8H3gSclmSGwaqyPwBuTrIFeAC4uDXfBVwATAPfBt4BUFUHk7wPuKO1e29VzS46eCeDFW8vBj7TNuY5hyRpQrqFTVVdephDbxnRtoDLDzPONmDbiPoU8NoR9cdHnUOSNDnHywIBSdISZthIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk2kqTuJhI2Sb6e5O4ke5NMtdrLk+xJsr+9Lm/1JLk2yXSSu5K8fmicza39/iSbh+pvaONPt75Z/E8pSZo1ySubn6mqdVW1vr2/Eri1qtYCt7b3AOcDa9u2FbgOBuEEXAWcA5wNXDUbUK3N1qF+G/t/HEnS4RxPt9E2Advb/nbgwqH6jhr4EnBqkjOA84A9VXWwqp4A9gAb27FTquqLVVXAjqGxJEkTMKmwKeDvktyZZGurnV5VDwO011e2+krgwaG+M602X31mRP05kmxNMpVk6sCBA8/zI0mSDmfZhM77xqp6KMkrgT1J/nmetqO+b6ljqD+3WHU9cD3A+vXrR7aRJD1/E7myqaqH2uujwKcYfOfySLsFRnt9tDWfAVYPdV8FPHSE+qoRdUnShCx62CT53iTfN7sPnAvcA+wEZleUbQZuafs7gcvaqrQNwJPtNttu4Nwky9vCgHOB3e3Y00k2tFVolw2NJUmagEncRjsd+FRbjbwM+Iuq+mySO4Cbk2wBHgAubu13ARcA08C3gXcAVNXBJO8D7mjt3ltVB9v+O4EbgRcDn2mbJGlCFj1squp+4CdG1B8H3jKiXsDlhxlrG7BtRH0KeO3znqwkaUEcT0ufJUlLlGEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd1N6i91SlpED7z3xyY9BR2HfvD/3L1o5/LKRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0ZNpKk7gwbSVJ3ho0kqTvDRpLUnWEjSerOsJEkdWfYSJK6M2wkSd0t2bBJsjHJV5NMJ7ly0vORpBPZkgybJCcBHwHOB84CLk1y1mRnJUknriUZNsDZwHRV3V9V/wncBGya8Jwk6YS1VMNmJfDg0PuZVpMkTcBS/UudGVGr5zRKtgJb29tvJflq11mdWE4DHpv0JI4H+cDmSU9B382fzVlXjfqv8qj90DiNlmrYzACrh96vAh6a26iqrgeuX6xJnUiSTFXV+knPQ5rLn83JWKq30e4A1iY5M8nJwCXAzgnPSZJOWEvyyqaqDiW5AtgNnARsq6p7JzwtSTphLcmwAaiqXcCuSc/jBObtSR2v/NmcgFQ953tzSZIW1FL9zkaSdBwxbLSgfEyQjldJtiV5NMk9k57Liciw0YLxMUE6zt0IbJz0JE5Uho0Wko8J0nGrqj4HHJz0PE5Uho0Wko8JkjSSYaOFNNZjgiSdeAwbLaSxHhMk6cRj2Ggh+ZggSSMZNlowVXUImH1M0D7gZh8TpONFko8DXwR+OMlMki2TntOJxCcISJK688pGktSdYSNJ6s6wkSR1Z9hIkrozbCRJ3Rk20gJL8ooke9v2zSTfGHp/cpK3JqkkPzLUZ02S/9/a3JdkR5IXDh0/O8k/JNmf5CtJ/jbJj7VjV885x94kbxva/1Z7EvfeJDsm8W8iufRZ6ijJ1cC3quoDQ7WbgTOAW6vq6lZbA/xNVb22PT17D3BDVX0syenA7cDbq+oLrf1PAadV1adHnWPOHP4B+K2qmuryIaUxeGUjLaIkLwXeCGxh8ISF56iqZ4Av8+xDTK8Ats8GTWvz+ar6dOfpSgvGsJEW14XAZ6vqX4CDSV4/t0GSFwHnAJ9tpdcAXznCuL85dNvstgWdsbQADBtpcV3K4O/80F4vHTr2qiR7gceBB6rqrlEDJLk9yb4kHxoqX1NV69r2M11mLj0Pho20SJK8Angz8GdJvg78NvC2JLN/muFfq2od8GpgQ5Kfa/V7gf+5Aqqqc4DfA162WHOXni/DRlo8FwE7quqHqmpNVa0Gvgb81HCjqnoYuBJ4Tyt9BPiVJD851OwlizFhaaEYNtLiuRT41JzaJ4G3j2j7aeAlSX66qr4JvA34v0mmk3yBQXB9eKj98Hc2e9vqNum44dJnSVJ3XtlIkrozbCRJ3Rk2kqTuDBtJUneGjSSpO8NGktSdYSNJ6s6wkSR199/h3nu8AZHyvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "app_train = pd.read_csv(\"../../../all/application_train.csv\")\n",
    "ax = sns.countplot(x='TARGET', data=app_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'Target' field distribution, we can see that there are about 90% of 0's and 10% 1's. If we keep accuracy as the metric, even a classifier that predicts 0's all the time will have a 90% accuracy. So we need a better metric to estimate the performance of  both the benchmark model and the solution model. The evaluation metric provided in the Kaggle competition is area under the ROC Curve (AUC-ROC). This is definetely a good evaluation metric considering the target variable bias as it takes into consideration the sensitivity and the specificity of the predictions.\n",
    "\n",
    "The Receiver Operating Characteristics curve (ROC) plots the False Positive Rate(FPR) vs True Postive Rate(TPR) with FPR on the x-axis and TPR on the y-axis. The area under this curve depicts the degree of seperablity between the two classes. So this metric measures how well the model is able to split the data into the right classes. If the AUC - ROC value is close to 1, it means the model is performing very well and if it close to 0 it means the model is performing poorly.\n",
    "\n",
    "The TPR value is the Recall or Sensitivity value which is calculated as below:\n",
    "$$TPR/Sensitivity/Recall = \\frac{TP}{(TP+FN)}$$\n",
    "\n",
    "The FPR value is complement of the specificity which is calculated as below:\n",
    "$$Specificity = \\frac{TN}{(TN+FP)}$$<br>\n",
    "$$FPR = (1 - Specificity)$$\n",
    "    $$= \\frac{FP}{(TN+FP)}$$\n",
    "\n",
    "We coud also use various other metrics as well like the F-score which tries to compare model with low precision and recall values. But I beleive based on this finance domain where mis-identifying false positives and false negatives both have impact on the companys revenue.\n",
    "\n",
    "### Project Design\n",
    "\n",
    "Below picture represents the design approach for the solving the Home Credit Default Risk Assessment Problem.<br>\n",
    "![Design Flow](Flow.png)\n",
    "\n",
    "**1. Exploratory Data Analysis**<br>\n",
    "Before using the data for building our solution it is critical that we have a complete understanding of the data we have inhand. The dictionary file provided along with the dataset would help us with understanding the fields, the content of the files and the relationship between the files. In addition we would need to understand the below:\n",
    "- Understand the dimensions of the data files and data type of the fields in each file\n",
    "- Get a sense of the missing values in the data so that we can see how to handle them later.\n",
    "- Profile the data fields to check the distrubution of the values, checking the distinct values, get the key statistics for each field\n",
    "- Check for correlation between fields as they could be either used to create a new feature or be handled correctly to not affect the models performance\n",
    "- Check for outliers and sparsity of the data as they will have significant impact on the models performance if not handled correctly.\n",
    "\n",
    "**2. Data Pre-processing**\n",
    "Based on the information we have from our EDA, we can then work on the below pre-processing steps.\n",
    "- Handling Missing Values: If there are NA's identified in the data depending on their count and on the field we can appropriately handle the same.\n",
    "- Removing non-pertinent and correlated fields: If we see there are highly correlated fields we can choose to elimate them except for one in the modelling. Also not every single field in the input files will have pertinent information. Based on our EDA, we can identify those fields and choose to eliminate them\n",
    "- Normalizing fields: Based on the distribution of the values in the field, we can see if they need to be normalised to make sure the feature are on appropriate scale for modelling\n",
    "- Transforming Fields: We may need to perform log transformations or create new features based on existing features to improve the model.\n",
    "- Perform One hot encoding of features - For the categorical features with multiple classes, one hot encoding will help the model errors assuming relationship between classes\n",
    "- Merging the data files: We need to merge the information in the different data files to feed it to the model.\n",
    "\n",
    "**3. Feature Importance and Selection**\n",
    "Once we have the data pre-processed, we can use the same to check which features are important so that they alone can be used for the modelling. This would make sure we dont have errors.\n",
    "- Calculate the feature importance in the merged files using feature importance parameter in a decision tree classifier\n",
    "- Select the features based on the importance for the model\n",
    "\n",
    "**4. Splitting of Data**\n",
    "We need to split the data for training, testing and validation sets so that it would help analyse and improve performance so as to acheive a bias-variance tradeoff\n",
    "- Splitting data into train test and cross validation sets use K-fold validation\n",
    "\n",
    "**4. Benchmark Model creation**\n",
    "We will create the benchmark model with Decision trees classifier as mentioned in the previous section. We will use the sklearn library for the same and since we are looking to predict probablites we will be using the predict_proba method for the same.\n",
    "\n",
    "**5. Benchmark Model Evaluation**\n",
    "We will then evaluate the benchmark model with the AUC-ROC metric and see how well we are performing on the training , cross-validation and testing sets. Based on the results we will make some minor adjustments to the model and see if we can improve the performance of the benchmark model further. If not we would call the same the baseline and move with the solution for the project.\n",
    "\n",
    "**6. Soultion based model creation**\n",
    "For predicting the credit defaulters, we will be predicting the probablities and ensemble techniques are very efficient for these problems. The common ensemble techniques are bagging and boosting. For this project, we would be checking on both the techniques and see which performs better.\n",
    "- Create a model using the Random Forest Classifier: This uses the bagging method and we will use the sklearn library\n",
    "- Create a model using Gradient Boosting Machine Classifier: This uses the boosting method and we will use the LightGBM library\n",
    "\n",
    "**7. Model Evaluvation**\n",
    "We will then evaluate the models with the AUC-ROC metric and see how well we are performing on the training , cross-validation and testing sets. We would plot the results to compare and analyse the performance of the two models on the various datasets. Based on analysis we will check which parameters need further tuning to imporve the performance of the model.\n",
    "\n",
    "**8. Hyper-parameter tunning**\n",
    "Hyper-parameter tunning is the way we can minimize the error and improve the performance of the model. We would employ two different methods to do the tuning.\n",
    "- For the Random Forest model we will use the Grid Search techinique to test different parameter sets.\n",
    "- For Gradient Boosting Machine we will use automated tuning using Bayesian optimization Techniques.\n",
    "\n",
    "*We will create functions for all process from step 4 through 8 and we create a pipeline to do more of a rinse and repeat of the steps till the model provides an much better improvement in performance over our score from the benchmark model.*\n",
    "\n",
    "**9. Prediction**\n",
    "Using the best model fitted we will predict the values for the test dataset provided by Kaggle and submit the same to see what our score is on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
